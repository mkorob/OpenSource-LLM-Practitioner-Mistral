{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AutoTokenizer, DataCollatorForSeq2Seq,\n\u001b[0;32m      7\u001b[0m                           LlamaForCausalLM, BitsAndBytesConfig, TrainingArguments, GenerationConfig)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import tqdm\n",
    "from transformers import (AutoTokenizer, DataCollatorForSeq2Seq,\n",
    "                          LlamaForCausalLM, BitsAndBytesConfig, TrainingArguments, GenerationConfig)\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from src.utils import load_dataset_task_prompt_mappings\n",
    "from dataload_utils import load_train_and_eval_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "WANDB_PROJECT_NAME = \"llama2_annotations_llm_comparison\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run LLAMA-2 models, you need to register yourself at the HuggingFace model page (https://huggingface.co/meta-llama/Llama-2-70b-chat-hf). Then, you can either insert the token here (not recommended if sharing a repository on GitHub), or input it in the hf_token.txt as done here and ensure it is included in the .gitignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(module_dir, \"hf_token.txt\"), \"r\") as file:\n",
    "    hf_token = file.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an optional parameter to run if your default transformers cache location does not contain enough storage to load the LLAMA models. Otherwise, you can keep it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache_location = os.environ['HF_HOME']\n",
    "cache_location = \"your/path/to/large/storage\"\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_location\n",
    "os.environ['HF_HOME'] = cache_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Arguments and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Variables\n",
    "\n",
    "# Type of task to run inference on\n",
    "task = 1  # Choices: [1,2,3,4,5,6]\n",
    "\n",
    "# Dataset to run inference on\n",
    "dataset = 1  # Choices: [1, 2, 3, 4]\n",
    "\n",
    "# Size of the sample to generate\n",
    "sample_size = '250'  # Choices: ['50','100','250','500','1000','1500']\n",
    "\n",
    "# Path to the directory to store the generated predictions\n",
    "output_dir = '../../data'\n",
    "\n",
    "# Path to the directory to store the models (make sure this location is included in the .gitignore if using GitHub)\n",
    "model_dir = '../../data'\n",
    "\n",
    "# Name of the model to finetune (this script was tested on LLAMA-2 70b, LLAMA-2 13b, and OASST-LLAMA 30b)\n",
    "model_name = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "\n",
    "#If using LLAMA2, keep True. Set False only for OASST-LLAMA.\n",
    "use_llama2_prompt = False\n",
    "\n",
    "#This is relevant for LLAMA2, where the system and user message are separated by context tokens. You should count how many lines your user message takes (in this case, 3)\n",
    "system_user_prompt_division_line = 3\n",
    "\n",
    "# Random seed to use\n",
    "seed = 2019\n",
    "\n",
    "# Path to the directory containing the datasets\n",
    "data_dir = '../../data'\n",
    "\n",
    "# Whether to use the full label\n",
    "not_use_full_labels = False\n",
    "\n",
    "# Path to the dataset-task mappings file\n",
    "dataset_task_mappings_fp = os.path.normpath(os.path.join(module_dir, '..', 'dataset_task_mappings.csv'))\n",
    "\n",
    "#Maximum length of prompt to be taken by the model as input (check documentation for current maximum length)\n",
    "max_prompt_len = 4096\n",
    "\n",
    "# Number of epochs to train the model\n",
    "n_epochs = 3\n",
    "\n",
    "# Batch size\n",
    "batch_size = 4\n",
    "\n",
    "#Gradient accumulation steps\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "#run name - Optional Argument if you want it to be called something else than the default way (defined below)\n",
    "run_name = \"\"\n",
    "\n",
    "#maximum length of sequence to produce\n",
    "max_new_tokens = 100\n",
    "\n",
    "### LoRA specific parameters\n",
    "temp = 0.05\n",
    "top_p = 0.75\n",
    "top_k = 40\n",
    "lora_config_alpha = 32\n",
    "lora_config_dropout = 0.05\n",
    "lora_config_r = 16\n",
    "lora_config_no_target_modules = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element, tokenizer, dataset_text_field, max_seq_len, batch_size):\n",
    "    outputs = tokenizer(\n",
    "        element[dataset_text_field],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=max_seq_len,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        padding='longest',\n",
    "        pad_to_multiple_of=batch_size\n",
    "    )\n",
    "\n",
    "    data_collator.collate_batch([outputs])\n",
    "\n",
    "    return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 123):\n",
    "    # tf.random.set_seed(123)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    # Set seed with the `transformers` library\n",
    "    # set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = run_name if run_name != '' else f'{model_name}_ds_{dataset}_task_{int(task)}_sample_{sample_size}_epochs_{n_epochs}_prompt_max_len_{max_prompt_len}_batch_size_{batch_size}_grad_acc_{gradient_accumulation_steps}'\n",
    "exp_name = exp_name.replace('.', '_')\n",
    "\n",
    "# Initialize the Weights and Biases run\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=exp_name,\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"model\": model_name,\n",
    "        \"dataset\": dataset,\n",
    "        \"task\": task,\n",
    "        \"epochs\": n_epochs,\n",
    "        \"max_prompt_len\": max_prompt_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not_use_full_labels:\n",
    "    exp_name += '_label_abbreviation'\n",
    "    labelset_col = 'labelset'\n",
    "else:\n",
    "    labelset_col = 'labelset_fullword'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running exp:', exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_col = 'zero_shot_prompt'\n",
    "# Load the prompt\n",
    "dataset_idx, dataset_task_mappings = load_dataset_task_prompt_mappings(\n",
    "    dataset_num=dataset, task_num=task, dataset_task_mappings_fp=dataset_task_mappings_fp)\n",
    "\n",
    "# Get information specific to the dataset\n",
    "label_column = dataset_task_mappings.loc[dataset_idx, \"label_column\"]\n",
    "labelset = dataset_task_mappings.loc[dataset_idx, labelset_col].split(\"; \")\n",
    "labelset = [label.strip() for label in labelset]\n",
    "prompt = dataset_task_mappings.loc[dataset_idx, prompt_col]\n",
    "\n",
    "# Get the system or instruction prompt and the user prompt format\n",
    "system_prompt = ('\\n'.join(prompt.split('\\n')[:-system_user_prompt_division_line])).strip()\n",
    "user_prompt_format = ('\\n'.join(prompt.split('\\n')[-system_user_prompt_division_line:])).strip()\n",
    "\n",
    "# Log the system prompt and user_prompt_format as files in wandb\n",
    "prompts_artifact = wandb.Artifact('prompts', type='prompts')\n",
    "with prompts_artifact.new_file('system_prompt.txt', mode='w') as f:\n",
    "    f.write(system_prompt)\n",
    "with prompts_artifact.new_file('user_prompt_format.txt', mode='w') as f:\n",
    "    f.write(user_prompt_format)\n",
    "wandb.run.log_artifact(prompts_artifact)\n",
    "\n",
    "# Load the train and eval datasets with the full prompt format\n",
    "print(f'label_columns: {label_column}')\n",
    "print(f'labelset: {labelset}')\n",
    "\n",
    "datasets = load_train_and_eval_datasets(\n",
    "    data_dir=data_dir, dataset_num=dataset, task_num=task,\n",
    "    label_column=label_column, labelset=labelset, full_label=not not_use_full_labels,\n",
    "    sample_size=sample_size, system_prompt=system_prompt, user_prompt_format=user_prompt_format,\n",
    "    llama_2=use_llama2_prompt, zero_shot_full_dataset=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print examples from train set, eval set and evalset without completion\n",
    "print(f\"Train set example with completion ({len(datasets['train'])} rows): \")\n",
    "print(\"-\" * 50 + '\\n')\n",
    "print(datasets[\"train\"][\"text\"][0])\n",
    "print('\\n\\n')\n",
    "\n",
    "print(f\"Eval set example with completion ({len(datasets['eval'])} rows): \")\n",
    "print(\"-\" * 50 + '\\n')\n",
    "print(datasets[\"eval\"][\"text\"][0])\n",
    "print('\\n\\n')\n",
    "\n",
    "print(f\"Eval set without completion ({len(datasets['eval_wo_completion'])} rows): \")\n",
    "print(\"-\" * 50 + '\\n')\n",
    "print(datasets[\"eval_wo_completion\"][\"text\"][0])\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, truncation_side=\"left\", use_fast=False, token=hf_token, cache_dir = \"../cache\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load the model\n",
    "use_4bit = True                         # Activate 4-bit precision base model loading\n",
    "bnb_4bit_compute_dtype = \"float16\"      # Compute dtype for 4-bit base models\n",
    "bnb_4bit_quant_type = \"nf4\"             # Quantization type (fp4 or nf4)\n",
    "use_nested_quant = False                # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, quantization_config=bnb_config,\n",
    "                                            device_map=\"auto\", token=hf_token, cache_dir = \"../cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Set LoRA configuration and add the adapters to  the model\n",
    "########################################################################\n",
    "\n",
    "# Add the adapter with the PeFT library\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_config_r, lora_alpha=lora_config_alpha, lora_dropout=lora_config_dropout,\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] if not lora_config_no_target_modules else None\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "########################################################################\n",
    "# Train the model and persist it\n",
    "########################################################################\n",
    "path_model_dir = os.path.join(model_dir, exp_name)\n",
    "\n",
    "# Define training args\n",
    "fp16 = False\n",
    "bf16 = True  # set bf16 to True with an A100\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    num_train_epochs=n_epochs,\n",
    "    load_best_model_at_end=False,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    report_to=[\"wandb\"]\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    max_seq_length=max_prompt_len,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "print(\"Training the model...\")\n",
    "trainer.train()\n",
    "trainer.save_model(path_model_dir)\n",
    "\n",
    "########################################################################\n",
    "# Merge LoRA weights with the model\n",
    "########################################################################\n",
    "# Free up memory to reload the model and merge the weight\n",
    "del trainer\n",
    "del model\n",
    "\n",
    "# Run garbage collector and empty cache manually\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "time.sleep(10)\n",
    "\n",
    "print(\"Merging LoRA weights...\")\n",
    "\n",
    "# Load base LLM model\n",
    "llama_model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    token=hf_token,\n",
    "    cache_dir = \"../cache\"\n",
    ")\n",
    "\n",
    "# It needs enough memory to load it in 16 bit, a little over 80GB, so more than 1 A100\n",
    "model = PeftModel.from_pretrained(\n",
    "    llama_model,\n",
    "    path_model_dir,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "merged_model_path = os.path.join(path_model_dir, \"merged_model\")\n",
    "print(f\"saving model merged with adapters in: {merged_model_path}\")\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(merged_model_path, safe_serialization=True)\n",
    "\n",
    "# Free up memory\n",
    "del llama_model\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "time.sleep(10)\n",
    "\n",
    "# Reload the model in its quantized version\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    merged_model_path, quantization_config=bnb_config, device_map=\"auto\", token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, truncation_side=\"left\", use_fast=False, token=hf_token)\n",
    "# tokenizer.pad_token = tokenizer.unk_token         # Uncomment if using UNK as padding token for batched inference\n",
    "# tokenizer.padding_side = \"left\"                   # Uncomment for batched inference\n",
    "\n",
    "# Default params from alpaca-lora generate script (commonly used)\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=temp,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=max_new_tokens\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions_out = []\n",
    "    for i, input_text_i in tqdm.tqdm(enumerate(datasets[\"eval_wo_completion\"][\"text\"])):\n",
    "        # Tokenize the text\n",
    "        tokenized_text_i = tokenizer(\n",
    "            text_target=input_text_i,\n",
    "            padding=False,\n",
    "            max_length=max_prompt_len,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Generate the completions\n",
    "        outputs = model.generate(\n",
    "            input_ids=tokenized_text_i[\"input_ids\"].cuda(),\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "\n",
    "        generated_text_minibatch = tokenizer.batch_decode(\n",
    "            outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "        predictions_out += generated_text_minibatch\n",
    "\n",
    "        if i == 0:\n",
    "            print(\"Sample prediction: \")\n",
    "            print(predictions_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dir = os.path.join(output_dir, 'predictions', os.path.basename(model_name),\n",
    "                                exp_name.replace(\"/\", \"_\"))\n",
    "os.makedirs(predictions_dir, exist_ok=True)\n",
    "file_name = f'ds_{dataset}__task_{task}_eval_set.csv'\n",
    "print(f'The filename that will be used is: {file_name}')\n",
    "eval_df = pd.read_csv(os.path.join(data_dir, file_name))\n",
    "eval_df['prediction_ds'] = predictions_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(os.path.join(\n",
    "    predictions_dir,\n",
    "    f'ds_{dataset}__task_{task}__sample_size_{sample_size}_eval_set.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_comparisons_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
