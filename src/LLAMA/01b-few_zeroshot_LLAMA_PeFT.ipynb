{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template for Zero-shot and Few-shot Classification with LLaMA through Low-Ranking Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:39:14.114062Z",
     "iopub.status.busy": "2024-07-23T14:39:14.113934Z",
     "iopub.status.idle": "2024-07-23T14:39:14.118037Z",
     "shell.execute_reply": "2024-07-23T14:39:14.117652Z",
     "shell.execute_reply.started": "2024-07-23T14:39:14.114046Z"
    }
   },
   "outputs": [],
   "source": [
    "#IMPORTANT - to ensure package loading, first add the path of the utils folder to your system path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(module_dir, os.pardir, \"utils\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:39:14.118748Z",
     "iopub.status.busy": "2024-07-23T14:39:14.118616Z",
     "iopub.status.idle": "2024-07-23T14:39:29.294554Z",
     "shell.execute_reply": "2024-07-23T14:39:29.294085Z",
     "shell.execute_reply.started": "2024-07-23T14:39:14.118733Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mkorob/conda/envs/environment_cp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from transformers import (AutoTokenizer,\n",
    "                          LlamaForCausalLM, BitsAndBytesConfig, GenerationConfig)\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from dataload_utils import load_full_dataset, load_dataset_task_prompt_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Arguments and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the following code block, you are asked to set up several key parameters that will define the behavior and environment of your fine-tuning process:\n",
    "\n",
    "1. **WandB Project Name (`WANDB_PROJECT_NAME`)**: This is the name of the project in Weights & Biases (WandB) where your training run will be logged. WandB is a tool that helps track experiments, visualize data, and share insights. By setting the project name here, you ensure that all the metrics, outputs, and logs from your training process are organized under a single project for easy access and comparison. Specify a meaningful name that reflects the nature of your training session or experiment. If you leave the argument empty, the project will not be tracked on WandB.\n",
    "\n",
    "2. **Model Name (`MODEL_NAME`)**: Here, you select the size of LLAMA model that you wish to fine-tune. This notebook was ran and tested on (`meta-llama/Llama-2-70b-chat-hf`, `meta-llama/Llama-2-13b-chat-hf` and `OASST-LLAMA 30b` (not available on HuggingFace anymore)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:39:29.295405Z",
     "iopub.status.busy": "2024-07-23T14:39:29.295126Z",
     "iopub.status.idle": "2024-07-23T14:39:29.298398Z",
     "shell.execute_reply": "2024-07-23T14:39:29.297368Z",
     "shell.execute_reply.started": "2024-07-23T14:39:29.295388Z"
    }
   },
   "outputs": [],
   "source": [
    "WANDB_PROJECT_NAME = \"llama3_annotations_llm_comparison\"\n",
    "# Name of the model to finetune (this script was tested on LLAMA-2 70b, LLAMA-2 13b, and OASST-LLAMA 30b)\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run LLAMA-2 models, you need to register yourself at the HuggingFace model page (https://huggingface.co/meta-llama/Llama-2-70b-chat-hf). Then, you can either insert the token here (not recommended if sharing a repository on GitHub), or input it in the hf_token.txt as done here and ensure it is included in the .gitignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:39:29.299232Z",
     "iopub.status.busy": "2024-07-23T14:39:29.298959Z",
     "iopub.status.idle": "2024-07-23T14:39:29.334062Z",
     "shell.execute_reply": "2024-07-23T14:39:29.333680Z",
     "shell.execute_reply.started": "2024-07-23T14:39:29.299215Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(module_dir, \"hf_token.txt\"), \"r\") as file:\n",
    "    hf_token = file.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an optional parameter to run if your default transformers cache location does not contain enough storage to load the LLAMA models. Otherwise, you can keep it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:39:29.334794Z",
     "iopub.status.busy": "2024-07-23T14:39:29.334583Z",
     "iopub.status.idle": "2024-07-23T14:39:29.337079Z",
     "shell.execute_reply": "2024-07-23T14:39:29.336608Z",
     "shell.execute_reply.started": "2024-07-23T14:39:29.334778Z"
    }
   },
   "outputs": [],
   "source": [
    "#cache_location = os.environ['HF_HOME']\n",
    "cache_location = \"../cache\"\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_location\n",
    "os.environ['HF_HOME'] = cache_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code block, you are required to set up various configuration variables that will dictate how the inference processes are executed. These variables are crucial as they define the nature of the task, the data, and the specific behaviors during the model's training and evaluation.\n",
    "\n",
    "1. **Task (`task`)**: Specify the type of task you want to run inference on. The task is represented by an integer, with each number corresponding to a different type of task (e.g., 1, 2, 3, etc.). You must select from the predefined choices, which are typically mapped to specific NLP tasks or scenarios.\n",
    "\n",
    "2. **Dataset (`dataset`)**: Choose the dataset on which you want to run inference. Like tasks, datasets are identified by integers, and each number corresponds to a different dataset. Ensure that the dataset selected is relevant to the task at hand.\n",
    "\n",
    "3. **Output Directory (`output_dir`)**: Define the path to the directory where you want to store the generated samples. This is where the output of your training and inference processes will be saved.\n",
    "\n",
    "4. **Model Directory (`model_dir`)**: Define the path to the directory where you want to store the generated models. This should have sufficient memory.\n",
    "\n",
    "5. **Llama-2 Prompt** (`use_llama2_prompt`)**: Keep True if running LLAMA-2 models (provides correct prompts with system and user message separated). Set False if running OASST models.\n",
    "\n",
    "6. **Division Line for User Message** (`system_user_prompt_division_line`)**: Length in number of lines of the user message. Relevant only for LLAMA-2 models.\n",
    "\n",
    "7. **Random Seed (`seed`)**: Setting a random seed ensures that the results are reproducible. By using the same seed, you can achieve the same outcomes on repeated runs under identical conditions.\n",
    "\n",
    "8. **Data Directory (`data_dir`)**: Specify the path to the directory containing the datasets you plan to use for training and evaluation.\n",
    "\n",
    "9. **Label Usage (`not_use_full_labels`)**: This boolean variable determines whether to use the full label descriptions or abbreviated labels during training and inference. Setting it to `False` means full labels will be used.\n",
    "\n",
    "10. **Dataset-Task Mappings File Path (`dataset_task_mappings_fp`)**: Define the path to the file containing mappings between datasets and tasks. This file is crucial for ensuring the correct dataset is used for the specified task.\n",
    "\n",
    "11. **Maximum prompt length (`max_prompt_len`)**: The maximum length of prompt in tokens to be taken as input before truncating the input. Longer input sequences require more computational power to run, so the shortest sequence required to capture the text is recommended.\n",
    "\n",
    "12. **Few shot indicator (`few_shot`)**: Set to True to run few-shot learning with the prompt defined as few_shot_prompt in the dataset mapping dataframe.\n",
    "\n",
    "13. **Batch size (`batch-size`)**: Number of observations used in each training and validation batch. Larger batch size requires more computational memory as one batch needs to fit on one machine, but makes learning more stable. We found that for FLAN-XL, batch size of 8 was possible by taking batch size of 4 and accumulating results of 2 batches (see  (`gradient_accumulation_steps`) below)\n",
    "\n",
    "14. **Gradient accumulation steps (`gradient_accumulation_steps`)**: In a case where gradient accumulation steps is larger than 1, instead of updating the gradient after each batch, the gradient is updated after the sum of _n_ batches. This allows to train a model to learn on a larger global batch (_batch size_ * _gradient accumulation steps_) than the one that is able to fit on one machine.\n",
    "\n",
    "15. **Run_name (`run_name`)**: Optional run_name to store in WandB. We recommend to keep it null, which generates an automatic run name based on all the relevant parameters of finetuning for easier tracking. \n",
    "\n",
    "16. **Maximum output length (`max_new_tokens`)**: Maximum length of prediction produced by LLAMA. For data labelling, it does not make sense to make it longer than that. \n",
    "\n",
    "17. **LoRA hyperparameters:** The values provided below were taken from Stanford Alpaca LoRA repository: https://github.com/tloen/alpaca-lora/blob/main/finetune.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:39:29.337670Z",
     "iopub.status.busy": "2024-07-23T14:39:29.337536Z",
     "iopub.status.idle": "2024-07-23T14:39:29.344845Z",
     "shell.execute_reply": "2024-07-23T14:39:29.344412Z",
     "shell.execute_reply.started": "2024-07-23T14:39:29.337655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration Variables\n",
    "\n",
    "# Type of task to run inference on\n",
    "task = 1  # \n",
    "\n",
    "# Dataset to run inference on\n",
    "dataset = 1  #\n",
    "\n",
    "# Path to the directory to store the generated predictions\n",
    "output_dir = '../../data'\n",
    "\n",
    "# Path to the directory to store the models (make sure this location is included in the .gitignore if using GitHub)\n",
    "model_dir = '../../data'\n",
    "\n",
    "#If using LLAMA2, keep True. Set False only for OASST-LLAMA.\n",
    "llama_type = \"llama3\"\n",
    "\n",
    "#This is relevant for LLAMA2, where the system and user message are separated by context tokens. You should count how many lines your user message takes (in this case, 3)\n",
    "system_user_prompt_division_line = 3\n",
    "\n",
    "# Random seed to use\n",
    "seed = 2019\n",
    "\n",
    "# Path to the directory containing the datasets\n",
    "data_dir = '../../data'\n",
    "\n",
    "# Whether to use the full label\n",
    "not_use_full_labels = False\n",
    "\n",
    "# Path to the dataset-task mappings file\n",
    "dataset_task_mappings_fp = os.path.normpath(os.path.join(module_dir, '..', '..', 'dataset_task_mappings.csv'))\n",
    "\n",
    "#Maximum length of prompt to be taken by the model as input (check documentation for current maximum length)\n",
    "max_prompt_len = 4096\n",
    "\n",
    "#Zero or few-shot binary variable\n",
    "few_shot = False\n",
    "\n",
    "#run name - Optional Argument if you want it to be called something else than the default way (defined below)\n",
    "run_name = \"\"\n",
    "\n",
    "#maximum length of sequence to produce\n",
    "max_new_tokens = 100\n",
    "\n",
    "#Text Generation parameters (Values below taken from Stanford Alpaca LoRA repository : https://github.com/tloen/alpaca-lora/blob/main/generate.py)\n",
    "temp = 0.05                                     \n",
    "top_p = 0.75      \n",
    "top_k = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:39:29.346952Z",
     "iopub.status.busy": "2024-07-23T14:39:29.346720Z",
     "iopub.status.idle": "2024-07-23T14:39:29.352319Z",
     "shell.execute_reply": "2024-07-23T14:39:29.351873Z",
     "shell.execute_reply.started": "2024-07-23T14:39:29.346936Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = f'ds_{dataset}__task_{task}_eval_set'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customizing for Your Own Tasks:**\n",
    "If you plan to run a custom task or use a dataset that is not predefined, you will need to make modifications to the `label_utils` file. This file contains all mappings for different datasets and tasks. Adding your custom task or dataset involves defining the new task or dataset number and specifying its characteristics and mappings in the `label_utils` file. This ensures that your custom task or dataset integrates seamlessly with the existing framework for training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:39:29.352983Z",
     "iopub.status.busy": "2024-07-23T14:39:29.352840Z",
     "iopub.status.idle": "2024-07-23T14:39:29.359412Z",
     "shell.execute_reply": "2024-07-23T14:39:29.358934Z",
     "shell.execute_reply.started": "2024-07-23T14:39:29.352968Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 123):\n",
    "    # tf.random.set_seed(123)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    # Set seed with the `transformers` library\n",
    "    # set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:39:29.360195Z",
     "iopub.status.busy": "2024-07-23T14:39:29.359946Z",
     "iopub.status.idle": "2024-07-23T14:39:29.372777Z",
     "shell.execute_reply": "2024-07-23T14:39:29.372345Z",
     "shell.execute_reply.started": "2024-07-23T14:39:29.360179Z"
    }
   },
   "outputs": [],
   "source": [
    "set_all_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:39:29.373592Z",
     "iopub.status.busy": "2024-07-23T14:39:29.373448Z",
     "iopub.status.idle": "2024-07-23T14:39:41.448597Z",
     "shell.execute_reply": "2024-07-23T14:39:41.447842Z",
     "shell.execute_reply.started": "2024-07-23T14:39:29.373576Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaria-korobeynikova\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/mkorob/OpenSource-LLM-Practitioner-Mistral/src/LLAMA/wandb/run-20240723_163930-sctgw2gz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maria-korobeynikova/llama3_annotations_llm_comparison/runs/sctgw2gz' target=\"_blank\">meta-llama/Meta-Llama-3-8B-Instruct_ds_1_task_1_sample_0_prompt_max_len_4096</a></strong> to <a href='https://wandb.ai/maria-korobeynikova/llama3_annotations_llm_comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maria-korobeynikova/llama3_annotations_llm_comparison' target=\"_blank\">https://wandb.ai/maria-korobeynikova/llama3_annotations_llm_comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maria-korobeynikova/llama3_annotations_llm_comparison/runs/sctgw2gz' target=\"_blank\">https://wandb.ai/maria-korobeynikova/llama3_annotations_llm_comparison/runs/sctgw2gz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = run_name if run_name != '' else f'{MODEL_NAME}_ds_{dataset}_task_{int(task)}_sample_{0}_prompt_max_len_{max_prompt_len}'\n",
    "if few_shot:\n",
    "    exp_name += \"few_shot\"\n",
    "exp_name = exp_name.replace('.', '_')\n",
    "\n",
    "# Initialize the Weights and Biases run\n",
    "if WANDB_PROJECT_NAME != \"\":\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=WANDB_PROJECT_NAME,\n",
    "        name=exp_name,\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"dataset\": dataset,\n",
    "            \"task\": task,\n",
    "            \"max_prompt_len\": max_prompt_len\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:39:41.449247Z",
     "iopub.status.busy": "2024-07-23T14:39:41.449104Z",
     "iopub.status.idle": "2024-07-23T14:39:41.460554Z",
     "shell.execute_reply": "2024-07-23T14:39:41.453995Z",
     "shell.execute_reply.started": "2024-07-23T14:39:41.449231Z"
    }
   },
   "outputs": [],
   "source": [
    "if not_use_full_labels:\n",
    "    exp_name += '_label_abbreviation'\n",
    "    labelset_col = 'labelset'\n",
    "else:\n",
    "    labelset_col = 'labelset_fullword'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:39:41.461282Z",
     "iopub.status.busy": "2024-07-23T14:39:41.461025Z",
     "iopub.status.idle": "2024-07-23T14:39:41.465904Z",
     "shell.execute_reply": "2024-07-23T14:39:41.465527Z",
     "shell.execute_reply.started": "2024-07-23T14:39:41.461266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running exp: meta-llama/Meta-Llama-3-8B-Instruct_ds_1_task_1_sample_0_prompt_max_len_4096\n"
     ]
    }
   ],
   "source": [
    "print('Running exp:', exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:39:41.466748Z",
     "iopub.status.busy": "2024-07-23T14:39:41.466507Z",
     "iopub.status.idle": "2024-07-23T14:39:41.716288Z",
     "shell.execute_reply": "2024-07-23T14:39:41.715842Z",
     "shell.execute_reply.started": "2024-07-23T14:39:41.466732Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_columns: relevant_ra\n",
      "labelset: ['RELEVANT', 'IRRELEVANT']\n",
      "loading ../../data/ds_1__task_1_eval_set.csv\n",
      "../../data/ds_1__task_1_eval_set.csv\n",
      "dataset has the following cols Index(['status_id', 'Date', 'text', 'relevant_ra'], dtype='object')\n",
      "The label_column is: relevant_ra\n",
      "loading ../../data/ds_1__task_1_eval_set.csv\n",
      "../../data/ds_1__task_1_eval_set.csv\n",
      "dataset has the following cols Index(['status_id', 'Date', 'text', 'relevant_ra'], dtype='object')\n",
      "The label_column is: relevant_ra\n"
     ]
    }
   ],
   "source": [
    "prompt_col = 'few_shot_prompt' if few_shot else 'zero_shot_prompt'\n",
    "# Load the prompt\n",
    "dataset_idx, dataset_task_mappings = load_dataset_task_prompt_mappings(\n",
    "    dataset_num=dataset, task_num=task, dataset_task_mappings_fp=dataset_task_mappings_fp)\n",
    "\n",
    "# Get information specific to the dataset\n",
    "label_column = dataset_task_mappings.loc[dataset_idx, \"label_column\"]\n",
    "labelset = dataset_task_mappings.loc[dataset_idx, labelset_col].split(\"; \")\n",
    "labelset = [label.strip() for label in labelset]\n",
    "prompt = dataset_task_mappings.loc[dataset_idx, prompt_col]\n",
    "\n",
    "# Get the system or instruction prompt and the user prompt format\n",
    "system_prompt = ('\\n'.join(prompt.split('\\n')[:-system_user_prompt_division_line])).strip()\n",
    "user_prompt_format = ('\\n'.join(prompt.split('\\n')[-system_user_prompt_division_line:])).strip()\n",
    "\n",
    "# Log the system prompt and user_prompt_format as files in wandb\n",
    "if WANDB_PROJECT_NAME != \"\":\n",
    "    prompts_artifact = wandb.Artifact('prompts', type='prompts')\n",
    "    with prompts_artifact.new_file('system_prompt.txt', mode='w') as f:\n",
    "        f.write(system_prompt)\n",
    "    with prompts_artifact.new_file('user_prompt_format.txt', mode='w') as f:\n",
    "        f.write(user_prompt_format)\n",
    "    wandb.run.log_artifact(prompts_artifact)\n",
    "\n",
    "# Load the train and eval datasets with the full prompt format\n",
    "print(f'label_columns: {label_column}')\n",
    "print(f'labelset: {labelset}')\n",
    "\n",
    "datasets = load_full_dataset(\n",
    "    data_dir=data_dir, dataset_name=dataset_name, task_num = task,\n",
    "    label_column=label_column, labelset=labelset, full_label=not not_use_full_labels, system_prompt=system_prompt, user_prompt_format=user_prompt_format,\n",
    "    llama_type=llama_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:40:04.233435Z",
     "iopub.status.busy": "2024-07-23T14:40:04.233103Z",
     "iopub.status.idle": "2024-07-23T14:40:04.240659Z",
     "shell.execute_reply": "2024-07-23T14:40:04.240221Z",
     "shell.execute_reply.started": "2024-07-23T14:40:04.233417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval set example with completion (387 rows): \n",
      "--------------------------------------------------\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>âContent moderationâ refers to the practice of screening and monitoring content posted by users on social media sites to determine if the content should be published or not, based on specific rules and guidelines. I will ask you to classify a tweet as RELEVANT or IRRELEVANT to the content moderation: A: Tweet is RELEVANT if it includes: social media platformsâ content moderation rules and practices, censorship, governmentsâ regulation of online content moderation, and/or mild forms of content moderation like flagging, shadowbanning, or account suspension. B: Tweet is IRRELEVANT if they do not refer to content moderation, as defined above. This would include, for example, a tweet by Trump that Twitter has labeled his tweet as âdisputedâ, or a tweet claiming that something is false.<|eot_id|><|start_header_id|>user<|end_header_id|>Now, is the following tweet RELEVANT or IRRELEVANT to content moderation? @jennahasredhair Aww ok didn't know sexy, yes I will report and block that account and follow your real one xxx<|eot_id|><|start_header_id|>assistant<|end_header_id|>RELEVANT<|eot_id|>\n",
      "\n",
      "\n",
      "\n",
      "Eval set without completion (387 rows): \n",
      "--------------------------------------------------\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>âContent moderationâ refers to the practice of screening and monitoring content posted by users on social media sites to determine if the content should be published or not, based on specific rules and guidelines. I will ask you to classify a tweet as RELEVANT or IRRELEVANT to the content moderation: A: Tweet is RELEVANT if it includes: social media platformsâ content moderation rules and practices, censorship, governmentsâ regulation of online content moderation, and/or mild forms of content moderation like flagging, shadowbanning, or account suspension. B: Tweet is IRRELEVANT if they do not refer to content moderation, as defined above. This would include, for example, a tweet by Trump that Twitter has labeled his tweet as âdisputedâ, or a tweet claiming that something is false.<|eot_id|><|start_header_id|>user<|end_header_id|>Now, is the following tweet RELEVANT or IRRELEVANT to content moderation? @jennahasredhair Aww ok didn't know sexy, yes I will report and block that account and follow your real one xxx<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Eval set example with completion ({len(datasets['eval'])} rows): \")\n",
    "print(\"-\" * 50 + '\\n')\n",
    "print(datasets[\"eval\"][\"text\"][0])\n",
    "print('\\n\\n')\n",
    "\n",
    "print(f\"Eval set without completion ({len(datasets['eval_wo_completion'])} rows): \")\n",
    "print(\"-\" * 50 + '\\n')\n",
    "print(datasets[\"eval_wo_completion\"][\"text\"][0])\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model, tokenizers, data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:40:34.168684Z",
     "iopub.status.busy": "2024-07-23T14:40:34.168260Z",
     "iopub.status.idle": "2024-07-23T14:41:40.099752Z",
     "shell.execute_reply": "2024-07-23T14:41:40.099284Z",
     "shell.execute_reply.started": "2024-07-23T14:40:34.168664Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:02<00:00, 15.57s/it]\n",
      "/data/mkorob/conda/envs/environment_cp/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, truncation_side=\"left\", use_fast = True, token=hf_token, cache_dir = \"../cache\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load the model\n",
    "use_4bit = True                         # Activate 4-bit precision base model loading\n",
    "bnb_4bit_compute_dtype = \"float16\"      # Compute dtype for 4-bit base models\n",
    "bnb_4bit_quant_type = \"nf4\"             # Quantization type (fp4 or nf4)\n",
    "use_nested_quant = False                # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config,\n",
    "                                            device_map=\"auto\", token=hf_token, cache_dir = \"../cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:41:40.100841Z",
     "iopub.status.busy": "2024-07-23T14:41:40.100602Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction: \n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>âContent moderationâ refers to the practice of screening and monitoring content posted by users on social media sites to determine if the content should be published or not, based on specific rules and guidelines. I will ask you to classify a tweet as RELEVANT or IRRELEVANT to the content moderation: A: Tweet is RELEVANT if it includes: social media platformsâ content moderation rules and practices, censorship, governmentsâ regulation of online content moderation, and/or mild forms of content moderation like flagging, shadowbanning, or account suspension. B: Tweet is IRRELEVANT if they do not refer to content moderation, as defined above. This would include, for example, a tweet by Trump that Twitter has labeled his tweet as âdisputedâ, or a tweet claiming that something is false.<|eot_id|><|start_header_id|>user<|end_header_id|>Now, is the following tweet RELEVANT or IRRELEVANT to content moderation? @jennahasredhair Aww ok didn't know sexy, yes I will report and block that account and follow your real one xxx<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I would classify this tweet as IRRELEVANT to content moderation. The tweet is about reporting and blocking an account, but it does not refer to the rules and practices of social media platforms' content moderation, censorship, governments' regulation of online content moderation, or mild forms of content moderation like flagging, shadowbanning, or account suspension.<|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231it [09:19,  2.10s/it]"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, truncation_side=\"left\", use_fast = False, token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Default params from alpaca-lora generate script (commonly used)\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=temp,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions_out = []\n",
    "    for i, input_text_i in tqdm.tqdm(enumerate(datasets[\"eval_wo_completion\"][\"text\"])):\n",
    "        # Tokenize the text\n",
    "        tokenized_text_i = tokenizer(\n",
    "            text_target=input_text_i,\n",
    "            padding=False,\n",
    "            max_length=max_prompt_len,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Generate the completions\n",
    "        outputs = model.generate(\n",
    "            input_ids=tokenized_text_i[\"input_ids\"].cuda(),\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "\n",
    "        generated_text_minibatch = tokenizer.batch_decode(\n",
    "            outputs, skip_special_tokens=False, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "        predictions_out += generated_text_minibatch\n",
    "\n",
    "        if i == 0:\n",
    "            print(\"Sample prediction: \")\n",
    "            print(predictions_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(output_dir, 'predictions', MODEL_NAME.replace(\"/\", \"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv to add the predictions on \n",
    "predictions_dir = os.path.join(output_dir, 'predictions', MODEL_NAME.replace(\"/\", \"_\"))\n",
    "os.makedirs(predictions_dir, exist_ok=True)\n",
    "eval_df = pd.read_csv(os.path.join(data_dir, f\"{dataset_name}.csv\"))\n",
    "eval_df['prediction'] = predictions_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(predictions_dir, f'{exp_name.replace(\"/\", \"_\")}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "eval_df.to_csv(os.path.join(predictions_dir, f'{exp_name.replace(\"/\", \"_\")}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WANDB_PROJECT_NAME != \"\":\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment_cp",
   "language": "python",
   "name": "environment_cp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
